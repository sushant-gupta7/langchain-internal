{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from llama_index import download_loader\n",
    "# from langchain.llms import OpenAI\n",
    "# import openai\n",
    "# from langchain.document_loaders import UnstructuredPDFLoader, OnlinePDFLoader\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# from langchain.vectorstores import Chroma, Pinecone\n",
    "# from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "# from pathlib import Path\n",
    "# import pinecone\n",
    "# from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "# from langchain.docstore.document import Document\n",
    "# from tqdm.auto import tqdm\n",
    "# import tiktoken\n",
    "# from pathlib import Path\n",
    "# from gpt_index import download_loader\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# from transformers import pipeline\n",
    "import nbformat\n",
    "from nbconvert.preprocessors import ExecutePreprocessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrurn_Data():\n",
    "    val = \"HHHHHHHHHHHHHHHHHHHHHHIIIIIIIIIIIIIIIIII\"\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HHHHHHHHHHHHHHHHHHHHHHIIIIIIIIIIIIIIIIII'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrurn_Data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDFReader = download_loader(\"PDFReader\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader = PDFReader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = loader.load_data(file=Path('./Spiritual Wealth.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # use cl100k_base tokenizer for gpt-3.5-turbo and gpt-4\n",
    "# tokenizer = tiktoken.get_encoding('cl100k_base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create the length function used by the RecursiveCharacterTextSplitter\n",
    "# def tiktoken_len(text):\n",
    "#     tokens = tokenizer.encode(\n",
    "#         text,\n",
    "#         disallowed_special=()\n",
    "#     )\n",
    "#     return len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_splitter = RecursiveCharacterTextSplitter(\n",
    "#     chunk_size=1000, chunk_overlap=200, length_function=tiktoken_len, separators=['\\n\\n', '\\n', ' ', ''])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from uuid import uuid4\n",
    "# from tqdm.auto import tqdm\n",
    "\n",
    "# chunks = []\n",
    "\n",
    "# for idx, record in enumerate(tqdm(data)):\n",
    "#     print(record)\n",
    "#     texts = text_splitter.split_text(record.text)\n",
    "#     chunks.extend([{\n",
    "#         'id': str(uuid4()),\n",
    "#         'text': texts[i],\n",
    "#         'chunk': i,\n",
    "#     } for i in range(len(texts))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPENAI_API_KEY = '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'\n",
    "# PINECONE_API_KEY = 'd26f38f0-4937-4f00-8591-a766922f09e8'\n",
    "# PINECONE_API_ENV = 'us-west4-gcp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai.api_key = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pinecone.init(\n",
    "#     api_key=PINECONE_API_KEY,\n",
    "#     environment=PINECONE_API_ENV # find next to API key in console\n",
    "# )\n",
    "\n",
    "# # check if 'openai' index already exists (only create index if not)\n",
    "# if 'langchain-internal-project' not in pinecone.list_indexes():\n",
    "#     pinecone.create_index('langchain-internal-project', dimension=1536)\n",
    "# # connect to index\n",
    "# index = pinecone.Index('langchain-internal-project')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed_model = \"text-embedding-ada-002\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm.auto import tqdm\n",
    "# import datetime\n",
    "# from time import sleep\n",
    "\n",
    "# batch_size = 100  # how many embeddings we create and insert at once\n",
    "\n",
    "# for i in tqdm(range(0, len(chunks), batch_size)):\n",
    "#     # find end of batch\n",
    "#     i_end = min(len(chunks), i+batch_size)\n",
    "#     meta_batch = chunks[i:i_end]\n",
    "#     # get ids\n",
    "#     ids_batch = [x['id']  for x in meta_batch]\n",
    "#     # get texts to encode\n",
    "#     texts = [x['text'] for x in meta_batch]\n",
    "#     # create embeddings (try-except added to avoid RateLimitError)\n",
    "#     try:\n",
    "#         res = openai.Embedding.create(input=texts, engine=embed_model)\n",
    "#     except:\n",
    "#         done = False\n",
    "#         while not done:\n",
    "#             sleep(5)\n",
    "#             try:\n",
    "#                 res = openai.Embedding.create(input=texts, engine=embed_model)\n",
    "#                 done = True\n",
    "#             except:\n",
    "#                 pass\n",
    "#     embeds = [record['embedding'] for record in res['data']]\n",
    "#     # cleanup metadata\n",
    "#     meta_batch = [{\n",
    "#         'text': x['text'],\n",
    "#         'chunk': x['chunk'],\n",
    "#         # 'url': x['url']\n",
    "#     } for x in meta_batch]\n",
    "#     to_upsert = list(zip(ids_batch, embeds, meta_batch))\n",
    "    # upsert to Pinecone\n",
    "    # # index.upsert(vectors=to_upsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upsert to Pinecone\n",
    "# index.upsert(vectors=to_upsert, namespace='example-namespace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whoami_data = pinecone.whoami()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whoami_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://langchain-internal-project-\" + whoami_data.projectname + \".svc.us-west4-gcp.pinecone.io\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit = 3750\n",
    "\n",
    "# def retrieve(query):\n",
    "#     res = openai.Embedding.create(\n",
    "#         input=[query],\n",
    "#         engine=embed_model\n",
    "#     )\n",
    "\n",
    "#     # retrieve from Pinecone\n",
    "#     xq = res['data'][0]['embedding']\n",
    "\n",
    "#     # get relevant contexts\n",
    "#     res = index.query(xq, top_k=3, include_metadata=True)\n",
    "#     contexts = [\n",
    "#         x['metadata']['text'] for x in res['matches']\n",
    "#     ]\n",
    "\n",
    "#     print(contexts)\n",
    "\n",
    "#     # build our prompt with the retrieved contexts included\n",
    "#     prompt_start = (\n",
    "#         \"Answer the question based on the context below.\\n\\n\"+\n",
    "#         \"Context:\\n\"\n",
    "#     )\n",
    "#     prompt_end = (\n",
    "#         f\"\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "#     )\n",
    "#     # append contexts until hitting limit\n",
    "    \n",
    "#     prompt = (prompt_start + \"\\n\\n---\\n\\n\".join(contexts) + prompt_end)\n",
    "\n",
    "#     return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def complete(prompt):\n",
    "#     # query text-davinci-003\n",
    "#     res = openai.Completion.create(\n",
    "#         engine='text-davinci-003',\n",
    "#         prompt=prompt,\n",
    "#         temperature=0,\n",
    "#         max_tokens=400,\n",
    "#         top_p=1,\n",
    "#         frequency_penalty=0,\n",
    "#         presence_penalty=0,\n",
    "#         stop=None\n",
    "#     )\n",
    "#     return res['choices'][0]['text'].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we retrieve relevant items from Pinecone\n",
    "# query = \"Can you summarize february 9 from this book\"\n",
    "# query_with_contexts = retrieve(query)\n",
    "# query_with_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then we complete the context-infused query\n",
    "# complete(query_with_contexts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "ea78c179b0752c4c1e5e979fea37274b233735e75425a098db911a939697f7db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
